# LlamaIndex Cloud Integration Guide

## Overview

The pipeline now includes **intelligent synchronization** with LlamaIndex Cloud as an optional Stage 6. This integration automatically uploads your embedded documents to LlamaIndex Cloud for vector search and retrieval.

### Key Features âœ¨

- **ğŸ”„ Intelligent Sync**: Only uploads new and updated documents, skips unchanged ones
- **ğŸ“Š Change Detection**: Uses content hashing to detect document changes
- **ğŸš€ Batch Upload**: Efficient batch processing for large datasets
- **âš¡ Optional**: Completely optional - enable only when needed
- **ğŸ›¡ï¸ Error Handling**: Robust retry logic and error reporting

## How It Works

### Pipeline Stages

When `USE_LLAMAINDEX=true`, the pipeline runs these stages:

1. **Scraping** â†’ 2. **Processing** â†’ 3. **Chunking** â†’ 4. **Embeddings** â†’ 5. **Neo4j** (optional) â†’ 6. **LlamaIndex Cloud** â­

### Change Detection

The pipeline tracks document changes using content hashing:

- **New documents** (change_status="new") â†’ Uploaded to LlamaIndex
- **Updated documents** (change_status="updated") â†’ Old version deleted, new version uploaded
- **Unchanged documents** (change_status="unchanged") â†’ Skipped (no upload needed)

This ensures your LlamaIndex Cloud storage stays perfectly in sync with your local data! ğŸ¯

## Setup Instructions

### 1. Get LlamaIndex Cloud Credentials

1. Sign up at [https://cloud.llamaindex.ai/](https://cloud.llamaindex.ai/)
2. Create a new project (or use an existing one)
3. Create or select a pipeline/index
4. Get your API key from the dashboard

### 2. Configure Environment Variables

Edit your `.env` file and add:

```bash
# Enable LlamaIndex Cloud integration
USE_LLAMAINDEX=true

# Your LlamaIndex Cloud credentials
LLAMACLOUD_API_KEY=llx-your-api-key-here
LLAMACLOUD_INDEX_NAME=your-pipeline-name

# Optional (usually not needed)
LLAMACLOUD_PROJECT_NAME=Default
LLAMACLOUD_ORGANIZATION_ID=your-org-id
LLAMACLOUD_BASE_URL=https://api.cloud.llamaindex.ai
```

### 3. Test Connection

Run the test script to verify your setup:

```bash
python scripts/test_llamaindex_sync.py
```

Expected output:
```
ğŸ§ª TESTING LLAMAINDEX CLOUD CONNECTION
âœ… Configuration loaded:
   Index Name: your-pipeline-name
   Project: Default
   Base URL: https://api.cloud.llamaindex.ai

âœ… Successfully connected to LlamaIndex Cloud!

ğŸ”„ Starting sync to LlamaIndex Cloud...
âœ… SYNC COMPLETED
   ğŸ“ New documents uploaded: X
   ğŸ”„ Documents updated: Y
   â­ï¸  Unchanged documents skipped: Z
```

### 4. Run Full Pipeline

Now run your pipeline normally:

```bash
python scripts/run_pipeline.py
```

The pipeline will automatically:
1. Scrape â†’ Process â†’ Chunk â†’ Generate Embeddings
2. Upload to LlamaIndex Cloud (Stage 6)
3. Show sync results in the logs

## Usage Examples

### Example 1: First Run (All New Documents)

```bash
# First time running with LlamaIndex enabled
python scripts/run_pipeline.py
```

Output:
```
â˜ï¸  STAGE 6/6: UPLOADING TO LLAMAINDEX CLOUD
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”„ Starting document sync to LlamaCloud index 'your-index'...
   Total local documents: 1500
   New documents: 1500
   Updated documents: 0
   Unchanged documents: 0 (will be skipped)

ğŸ“¤ Uploading 1500 new documents...
âœ… Successfully uploaded 100 documents
âœ… Successfully uploaded 100 documents
... (continues in batches of 100)

âœ… SYNC COMPLETED
   ğŸ“ New documents uploaded: 1500
   ğŸ”„ Documents updated: 0
   â­ï¸  Unchanged documents skipped: 0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Example 2: Second Run (Only Changed Documents)

```bash
# Run again after website content has changed
python scripts/run_pipeline.py
```

Output:
```
â˜ï¸  STAGE 6/6: UPLOADING TO LLAMAINDEX CLOUD
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”„ Starting document sync to LlamaCloud index 'your-index'...
   Total local documents: 1550
   New documents: 60 (new pages)
   Updated documents: 15 (content changed)
   Unchanged documents: 1475 (will be skipped)

ğŸ“¤ Uploading 60 new documents...
âœ… Successfully uploaded 60 documents

ğŸ”„ Updating 15 documents...
âœ… Successfully deleted 15 documents
âœ… Successfully uploaded 15 documents

âœ… SYNC COMPLETED
   ğŸ“ New documents uploaded: 60
   ğŸ”„ Documents updated: 15
   â­ï¸  Unchanged documents skipped: 1475
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**Result**: Only 75 documents uploaded (60 new + 15 updated), saving time and API costs! ğŸ’°

## Sync Behavior Details

### How Sync Works

The `sync_documents()` method intelligently handles all scenarios:

| Change Status | Action | Description |
|--------------|--------|-------------|
| `new` | âœ… Upload | First time seeing this document - upload to LlamaIndex |
| `updated` | ğŸ”„ Replace | Content changed - delete old version, upload new version |
| `unchanged` | â­ï¸ Skip | Content identical - skip upload (already in LlamaIndex) |

### Metadata Preservation

Each document uploaded includes rich metadata:

```python
{
    "text": "document content...",
    "metadata": {
        "source_url": "https://...",
        "document_title": "Page Title",
        "chunk_id": "unique-chunk-identifier",
        "doc_id": "document-url",
        "change_status": "new|updated|unchanged",
        "scraped_at": "2025-11-05T15:26:26.096668",
        "article_id": "200573210",
        "chunk_index": 0,
        "chunk_title": "Section Title"
        # ... and more
    },
    "id": "unique-chunk-identifier",
    "embedding": [0.123, 0.456, ...] (384 or 768 dimensions)
}
```

This metadata enables powerful filtering and search in LlamaIndex! ğŸ”

## Disabling LlamaIndex

To disable LlamaIndex and skip Stage 6:

```bash
# In .env file
USE_LLAMAINDEX=false
```

Or simply leave it unset (default is `false`).

The pipeline will skip the LlamaIndex upload stage and continue to work normally.

## Troubleshooting

### Issue: "Failed to connect to LlamaCloud"

**Solution**: Check your credentials:
```bash
# Run test script for diagnostics
python scripts/test_llamaindex_sync.py
```

Verify in `.env`:
- `LLAMACLOUD_API_KEY` is correct (starts with `llx-`)
- `LLAMACLOUD_INDEX_NAME` matches your pipeline name
- No extra spaces or quotes

### Issue: "Index name is required"

**Solution**: Set your index/pipeline name:
```bash
LLAMACLOUD_INDEX_NAME=your-pipeline-name
```

### Issue: "API rate limit exceeded"

**Solution**: The pipeline uses batch uploads (100 documents per batch) with retry logic. If you hit rate limits:
1. Reduce batch size in the code (default: 100)
2. Wait a few minutes and retry
3. Check your LlamaIndex Cloud plan limits

### Issue: Documents not appearing in LlamaIndex Cloud

**Checklist**:
1. âœ… Check logs for "âœ… SYNC COMPLETED" message
2. âœ… Verify sync results show uploaded documents: `ğŸ“ New documents uploaded: X`
3. âœ… Wait 1-2 minutes for indexing (LlamaIndex needs time to process)
4. âœ… Refresh your LlamaIndex Cloud dashboard
5. âœ… Check the correct project/pipeline is selected

## API Reference

### PipelineOrchestrator

#### `upload_to_llamaindex(chunks_with_embeddings)`

Uploads chunks with embeddings to LlamaIndex Cloud.

```python
from src.pipeline.orchestrator import PipelineOrchestrator

orchestrator = PipelineOrchestrator()
result = orchestrator.upload_to_llamaindex(chunks_with_embeddings)

print(f"Uploaded: {result['new_count']}")
print(f"Updated: {result['updated_count']}")
```

### LlamaIndexClient

#### `sync_documents(documents, batch_size=100)`

Intelligently syncs documents to LlamaIndex Cloud.

```python
from src.integrations.llamaindex.client import LlamaIndexClient

client = LlamaIndexClient()
client.connect()

result = client.sync_documents(documents, batch_size=100)
# Returns: {"new_count": X, "updated_count": Y, "unchanged_count": Z, ...}

client.disconnect()
```

#### `upload_documents(documents)`

Directly uploads documents (no change detection).

```python
client.upload_documents(documents)
```

#### `delete_documents(document_ids=None, metadata_filter=None)`

Deletes documents by IDs or metadata filter.

```python
# Delete by IDs
client.delete_documents(document_ids=["id1", "id2", "id3"])

# Delete by metadata filter
client.delete_documents(metadata_filter={"article_id": "200573210"})
```

## Performance Tips

### 1. Batch Size Tuning

Default batch size is 100. Adjust based on your needs:

```python
# In your code
result = client.sync_documents(documents, batch_size=50)  # Smaller batches
```

### 2. Concurrent Processing

Use streaming mode for faster processing:

```bash
# In .env
PIPELINE_MODE=streaming
```

This processes documents as they're scraped, reducing total time.

### 3. Incremental Updates

Run the pipeline regularly (daily/weekly) to:
- âœ… Only sync changed documents
- âœ… Save API calls and costs
- âœ… Keep LlamaIndex Cloud up-to-date automatically

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Pipeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Stage 1: Scraping       â†’  data/raw/                       â”‚
â”‚  Stage 2: Processing     â†’  data/processed/                 â”‚
â”‚  Stage 3: Chunking       â†’  data/chunks/                    â”‚
â”‚  Stage 4: Embeddings     â†’  data/embeddings/                â”‚
â”‚  Stage 5: Neo4j (opt)    â†’  Neo4j Graph Database            â”‚
â”‚  Stage 6: LlamaIndex â­  â†’  LlamaIndex Cloud                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   LlamaIndex Cloud    â”‚
                  â”‚   Vector Storage      â”‚
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â”‚  â€¢ Vector Search      â”‚
                  â”‚  â€¢ Semantic Retrieval â”‚
                  â”‚  â€¢ RAG Applications   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Next Steps

After setting up LlamaIndex integration:

1. **Query Your Data**: Use LlamaIndex Cloud dashboard to query your documents
2. **Build RAG Apps**: Integrate LlamaIndex into your applications
3. **Monitor Syncs**: Check logs for sync statistics and errors
4. **Automate**: Set up scheduled pipeline runs (cron, GitHub Actions, etc.)

## Support

For issues or questions:

- **Pipeline Issues**: Check logs in `logs/pipeline.log`
- **LlamaIndex Docs**: [https://docs.llamaindex.ai/](https://docs.llamaindex.ai/)
- **API Reference**: [https://docs.cloud.llamaindex.ai/](https://docs.cloud.llamaindex.ai/)

---

**Happy syncing! ğŸš€**


