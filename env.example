# ============================================================================
# Pinecone Configuration (REQUIRED)
# ============================================================================
# Get your credentials from: https://app.pinecone.io/
# Set USE_PINECONE to "true" to enable Pinecone vector storage
# Note: Your index dimension must match your embedding model's output dimension
USE_PINECONE=true
PINECONE_API_KEY=your-pinecone-api-key-here
PINECONE_ENVIRONMENT=us-west1-gcp
PINECONE_INDEX_NAME=your-index-name
PINECONE_NAMESPACE=

# ============================================================================
# Storage Configuration
# ============================================================================
# Storage mode for pipeline data:
# - "local": Store data only in local data/ directory (default, no cloud needed)
# - "s3": Store data in AWS S3 (requires S3 credentials below)
# - "auto": Use S3 if configured, otherwise local (smart fallback)
STORAGE_MODE=local

# Pipeline processing mode:
# - "batch": Wait for all scraping to complete, then process sequentially
# - "streaming": Process items as they're scraped for concurrent execution (FASTER!)
PIPELINE_MODE=streaming

# ============================================================================
# AWS S3 Configuration (Optional - for data backup)
# ============================================================================
S3_BUCKET_NAME=your-s3-bucket-name
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key
AWS_REGION=us-east-1

# ============================================================================
# Embedding Configuration
# ============================================================================
# Choose your embedding provider:
# - "sentence-transformers" (local, no API key needed) - RECOMMENDED
# - "ollama" (local Ollama server, no API key needed)
# - "openai" (OpenAI API or compatible service)
EMBEDDING_PROVIDER=sentence-transformers

# Model for sentence-transformers:
# - BAAI/bge-small-en-v1.5 (384-dim, high quality) - RECOMMENDED
# - all-MiniLM-L6-v2 (384-dim, fast)
# - all-mpnet-base-v2 (768-dim, quality)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
EMBEDDING_BATCH_SIZE=32

# Note: BAAI/bge-small-en-v1.5 produces 384-dimensional vectors
# Your Pinecone index MUST be created with dimension=384 to match

# ============================================================================
# Option 2: Ollama (Local Server)
# ============================================================================
# Uncomment to use Ollama:
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_BASE_URL=http://localhost:11434
# EMBEDDING_BATCH_SIZE=16

# ============================================================================
# Option 3: OpenAI-Compatible API
# ============================================================================
# Uncomment to use OpenAI or compatible APIs:
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_API_KEY=your-api-key-here
# OPENAI_API_BASE=https://api.openai.com/v1
# OPENAI_ORG_ID=  # Optional
# EMBEDDING_BATCH_SIZE=100

# ============================================================================
# Pipeline Configuration
# ============================================================================
DATA_DIR=./data
LOG_LEVEL=INFO

# ============================================================================
# Scraper Configuration
# ============================================================================
SCRAPER_DOWNLOAD_DELAY=1.0
SCRAPER_CONCURRENT_REQUESTS=2
SCRAPER_DEPTH_LIMIT=4

# ============================================================================
# Chunking Configuration
# ============================================================================
CHUNK_SIZE=512
CHUNK_OVERLAP=64

# ============================================================================
# CSV Export Configuration
# ============================================================================
CSV_INCLUDE_EMBEDDINGS=true
CSV_OUTPUT_DIR=./data/csv_export

# ============================================================================
# Neo4j Configuration (OPTIONAL - Disabled by Default)
# ============================================================================
# Enable Neo4j vector database integration
# Set to "true" to enable Neo4j storage (Stage 5 of pipeline)
# Set to "false" to skip Neo4j and only use local/S3 storage
USE_NEO4J=false

# Neo4j credentials (only required if USE_NEO4J=true)
# NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your-password-here
# NEO4J_DATABASE=neo4j

# ============================================================================
# LlamaIndex Cloud Configuration (OPTIONAL - Disabled by Default)
# ============================================================================
# Enable LlamaIndex Cloud vector storage integration
# Set to "true" to enable LlamaIndex Cloud upload (Stage 6 of pipeline)
# Set to "false" to skip LlamaIndex upload
USE_LLAMAINDEX=false

# LlamaIndex Cloud credentials (only required if USE_LLAMAINDEX=true)
# Get your API key from: https://cloud.llamaindex.ai/
# LLAMACLOUD_API_KEY=llx-...
# LLAMACLOUD_INDEX_NAME=your-index-name
# LLAMACLOUD_PROJECT_NAME=Default
# LLAMACLOUD_ORGANIZATION_ID=your-org-id
# LLAMACLOUD_BASE_URL=https://api.cloud.llamaindex.ai
